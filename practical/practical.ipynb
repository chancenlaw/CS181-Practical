{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the training and validation datasets\n",
    "train = pd.read_csv('data/train.csv')  # Training data\n",
    "val = pd.read_csv('data/val.csv')      # Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression (CountVectorizer) ===\n",
      "Train Accuracy: 0.9851557389422835\n",
      "Validation Accuracy: 0.5172754195459033\n",
      "\n",
      "Per-Class Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1TV       0.00      0.00      0.00         0\n",
      "       ALJAZ       0.43      0.21      0.28        29\n",
      "     BBCNEWS       0.67      0.63      0.65       271\n",
      "   BELARUSTV       0.00      0.00      0.00         0\n",
      "   BLOOMBERG       0.69      0.84      0.76       170\n",
      "        CNBC       0.67      0.70      0.68       256\n",
      "        CNNW       0.42      0.54      0.47       219\n",
      "         COM       0.00      0.00      0.00         0\n",
      "       CSPAN       0.43      0.48      0.45       204\n",
      "      CSPAN2       0.25      0.42      0.32       165\n",
      "      CSPAN3       0.31      0.15      0.21       177\n",
      "          DW       0.62      0.46      0.53        46\n",
      "         FBC       0.63      0.69      0.66       214\n",
      "    FOXNEWSW       0.58      0.49      0.53       250\n",
      "         GBN       0.22      0.79      0.35        14\n",
      "        KDTV       0.88      0.72      0.79        32\n",
      "         KGO       0.47      0.38      0.42       141\n",
      "        KNTV       0.37      0.34      0.36       185\n",
      "        KPIX       0.35      0.25      0.29        59\n",
      "        KQED       0.67      0.11      0.19        18\n",
      "        KRON       0.65      0.52      0.58       151\n",
      "        KSTS       0.68      0.86      0.76        22\n",
      "        KTVU       0.41      0.48      0.44       115\n",
      "      LINKTV       0.00      0.00      0.00         0\n",
      "      MSNBCW       0.58      0.62      0.60       223\n",
      "         NTV       0.00      0.00      0.00         0\n",
      "     PRESSTV       0.00      0.00      0.00         0\n",
      "          RT       0.00      0.00      0.00        41\n",
      "     RUSSIA1       0.00      0.00      0.00         0\n",
      "    RUSSIA24       0.00      0.00      0.00         0\n",
      "       SFGTV       0.76      0.35      0.48        37\n",
      "\n",
      "    accuracy                           0.52      3039\n",
      "   macro avg       0.38      0.36      0.35      3039\n",
      "weighted avg       0.52      0.52      0.51      3039\n",
      "\n",
      "==================================================\n",
      "=== Logistic Regression (TFIDF) ===\n",
      "Train Accuracy: 0.8421979570271222\n",
      "Validation Accuracy: 0.5409674234945706\n",
      "\n",
      "Per-Class Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1TV       0.00      0.00      0.00         0\n",
      "       ALJAZ       0.83      0.17      0.29        29\n",
      "     BBCNEWS       0.58      0.77      0.66       271\n",
      "   BELARUSTV       0.00      0.00      0.00         0\n",
      "   BLOOMBERG       0.63      0.86      0.73       170\n",
      "        CNBC       0.68      0.67      0.68       256\n",
      "        CNNW       0.38      0.71      0.50       219\n",
      "         COM       0.00      0.00      0.00         0\n",
      "       CSPAN       0.44      0.52      0.48       204\n",
      "      CSPAN2       0.28      0.46      0.35       165\n",
      "      CSPAN3       0.23      0.05      0.08       177\n",
      "          DW       0.53      0.20      0.29        46\n",
      "         FBC       0.66      0.70      0.68       214\n",
      "    FOXNEWSW       0.65      0.53      0.58       250\n",
      "         GBN       0.23      0.86      0.36        14\n",
      "        KDTV       0.85      0.69      0.76        32\n",
      "         KGO       0.61      0.35      0.44       141\n",
      "        KNTV       0.63      0.39      0.49       185\n",
      "        KPIX       0.60      0.15      0.24        59\n",
      "        KQED       1.00      0.06      0.11        18\n",
      "        KRON       0.67      0.50      0.57       151\n",
      "        KSTS       0.66      0.86      0.75        22\n",
      "        KTVU       0.52      0.57      0.54       115\n",
      "      LINKTV       0.00      0.00      0.00         0\n",
      "      MSNBCW       0.68      0.61      0.64       223\n",
      "         NTV       0.00      0.00      0.00         0\n",
      "     PRESSTV       0.00      0.00      0.00         0\n",
      "          RT       0.00      0.00      0.00        41\n",
      "     RUSSIA1       0.00      0.00      0.00         0\n",
      "    RUSSIA24       0.00      0.00      0.00         0\n",
      "       SFGTV       0.83      0.27      0.41        37\n",
      "\n",
      "    accuracy                           0.54      3039\n",
      "   macro avg       0.42      0.35      0.34      3039\n",
      "weighted avg       0.56      0.54      0.52      3039\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Encode the labels\n",
    "# Convert channel names (strings) into numerical labels using LabelEncoder()\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train['channel'])  # Fit on training labels\n",
    "y_val = le.transform(val['channel'])          # Transform validation labels using the same encoder\n",
    "\n",
    "# Helper function to evaluate model performance\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "\n",
    "    print(f\"=== {model_name} ===\")\n",
    "    print(\"Train Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "    print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "\n",
    "    # Evaluate across all possible classes, even if some are missing in val set\n",
    "    print(\"\\nPer-Class Report (Validation):\")\n",
    "    print(classification_report(\n",
    "        y_val,\n",
    "        y_val_pred,\n",
    "        labels=range(len(le.classes_)),\n",
    "        target_names=le.classes_,\n",
    "        zero_division=0  # Avoid divide-by-zero warnings for empty classes\n",
    "    ))\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Feature extraction using CountVectorizer\n",
    "# Convert text transcripts into a sparse matrix of token counts\n",
    "count_vec = CountVectorizer(max_features=10000)  # Limit vocabulary size to 10,000 for performance\n",
    "X_train_count = count_vec.fit_transform(train['snip'])  # Fit + transform on training transcripts\n",
    "X_val_count = count_vec.transform(val['snip'])          # Only transform on validation transcripts\n",
    "\n",
    "# Train and evaluate Logistic Regression on CountVectorizer features\n",
    "lr_count = LogisticRegression(max_iter=1000)\n",
    "evaluate_model(lr_count, X_train_count, y_train, X_val_count, y_val, \"Logistic Regression (CountVectorizer)\")\n",
    "\n",
    "# Feature extraction using TfidfVectorizer\n",
    "# Convert text transcripts into TF-IDF-weighted features\n",
    "tfidf_vec = TfidfVectorizer(max_features=10000)  # Also limit vocabulary size here\n",
    "X_train_tfidf = tfidf_vec.fit_transform(train['snip'])  # Fit + transform on training transcripts\n",
    "X_val_tfidf = tfidf_vec.transform(val['snip'])          # Only transform on validation transcripts\n",
    "\n",
    "# Train and evaluate Logistic Regression on TF-IDF features\n",
    "lr_tfidf = LogisticRegression(max_iter=1000)  # Same model, different feature representation\n",
    "evaluate_model(lr_tfidf, X_train_tfidf, y_train, X_val_tfidf, y_val, \"Logistic Regression (TFIDF)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall objective of this model is to classify cable news clips by predicting which channel (e.g., CNN, FOX, MSNBC, etc.) a given transcript snippet came from. To achieve this, the model uses logistic regression, a linear classification algorithm that learns to associate input features with probability scores for each possible class. In this context, the input features are generated from the raw transcript text using two common text vectorization methods: CountVectorizer, which represents each clip as a vector of word counts, and TfidfVectorizer, which scales word counts by their importance across documents. The logistic regression model is trained by minimizing a loss function (cross-entropy) that penalizes incorrect class predictions, adjusting model weights using gradient descent to improve prediction accuracy on the training data. Once trained, the model outputs a probability distribution over the possible classes for each input and selects the most likely one as its prediction. The evaluation focuses on measuring how well the model generalizes to unseen data (the validation set), using both overall accuracy and detailed per-class metrics to understand performance. The ultimate goal is to build a model that can accurately and reliably classify unseen transcript snippets into the correct news channel based solely on their textual content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Accuracy\n",
    "Logistic Regression (CountVectorizer)\n",
    "\n",
    "Train Accuracy: 98.5%\n",
    "Validation Accuracy: 51.7%\n",
    "Logistic Regression (TF-IDF)\n",
    "Train Accuracy: 84.2%\n",
    "Validation Accuracy: 54.1%\n",
    "\n",
    "The CountVectorizer model achieved high training accuracy but significantly lower validation accuracy, indicating overfitting. In contrast, the TF-IDF model had a better train-validation balance and slightly improved validation performance, suggesting better generalization.\n",
    "\n",
    "Per-Class Observations\n",
    "Strong performing classes included:\n",
    "BLOOMBERG, CNBC, KSTS, FOXNEWSW, and KDTV.\n",
    "These had relatively high precision and recall, likely due to distinctive vocabulary or content.\n",
    "\n",
    "Moderate performance was observed for mainstream channels such as:\n",
    "BBCNEWS, CNNW, MSNBCW, and FBC.\n",
    "These may share overlapping topics, making them harder to distinguish.\n",
    "\n",
    "Poorly performing or misclassified classes included:\n",
    "GBN, KQED, KPIX, DW, and RT, where the model struggled to make accurate predictions.\n",
    "\n",
    "Some channels like 1TV, BELARUSTV, LINKTV, and RUSSIA1 had no support in the validation set, meaning they did not appear and could not be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for cleaning text\n",
    "def clean_html(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    # Remove HTML tags\n",
    "    clean = re.sub(r'<.*?>', '', str(text))\n",
    "    # Remove extra whitespaces\n",
    "    clean = re.sub(r'\\s+', ' ', clean).strip()\n",
    "    # Replace HTML entities\n",
    "    clean = re.sub(r'&amp;', '&', clean)\n",
    "    clean = re.sub(r'&lt;', '<', clean)\n",
    "    clean = re.sub(r'&gt;', '>', clean)\n",
    "    clean = re.sub(r'&quot;|&#34;', '\"', clean)\n",
    "    clean = re.sub(r'&apos;|&#39;', \"'\", clean)\n",
    "    return clean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
